{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising-recontruction Autoencoder (DRACO) Visualizer for particle picking downstream task\n",
    "\n",
    "In this demo, we will show how to pick particles using finetuned DRACO model in detectron2.The main architechture of particle picking is based on Detectron2(<https://github.com/facebookresearch/detectron2>) with some customized modification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from IPython import get_ipython\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image, ImageDraw\n",
    "from process import preprocess\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from draco.configuration import CfgNode\n",
    "\n",
    "from detectron.layers.layers import ShapeSpec\n",
    "from detectron.rcnn import GeneralizedRCNN\n",
    "from detectron.backbone import ViT, SimpleFeaturePyramid\n",
    "from detectron.layers.fpn import LastLevelMaxPool\n",
    "from detectron.layers.anchor_generator import DefaultAnchorGenerator\n",
    "from detectron.layers.box_regression import Box2BoxTransform\n",
    "from detectron.layers.matcher import Matcher\n",
    "from detectron.layers.pooler import ROIPooler\n",
    "from detectron.rpn import RPN, StandardRPNHead\n",
    "from detectron.roi_heads import ROIHeads, StandardROIHeads\n",
    "from detectron.fast_rcnn import FastRCNNOutputLayers\n",
    "from detectron.box_head import FastRCNNConvFCHead\n",
    "import detectron.transforms.augmentation_impl as T\n",
    "from detectron.transforms.augmentation import AugmentationList, AugInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg):\n",
    "    model = GeneralizedRCNN(\n",
    "        backbone=SimpleFeaturePyramid(\n",
    "            net=ViT(\n",
    "                img_size=cfg.MODEL.BACKBONE.NET.IMG_SIZE,\n",
    "                patch_size=cfg.MODEL.BACKBONE.NET.PATCH_SIZE,\n",
    "                in_chans=cfg.MODEL.BACKBONE.NET.IN_CHANS,\n",
    "                embed_dim=cfg.MODEL.BACKBONE.NET.EMBED_DIM,\n",
    "                depth=cfg.MODEL.BACKBONE.NET.DEPTH,\n",
    "                num_heads=cfg.MODEL.BACKBONE.NET.NUM_HEADS,\n",
    "                drop_path_rate=cfg.MODEL.BACKBONE.NET.DROP_PATH_RATE,\n",
    "                window_size=cfg.MODEL.BACKBONE.NET.WINDOW_SIZE,\n",
    "                mlp_ratio=4,\n",
    "                qkv_bias=True,\n",
    "                norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "                window_block_indexes=cfg.MODEL.BACKBONE.NET.WINDOW_BLOCK_INDEXES,\n",
    "                residual_block_indexes=[],\n",
    "                use_rel_pos=True,\n",
    "                out_feature=\"last_feat\",\n",
    "            ),\n",
    "            in_feature=\"last_feat\",\n",
    "            out_channels=cfg.MODEL.BACKBONE.OUT_DIM,\n",
    "            scale_factors=cfg.MODEL.BACKBONE.SCALES,\n",
    "            top_block=LastLevelMaxPool(),\n",
    "            norm=\"LN\",\n",
    "            square_pad=cfg.MODEL.BACKBONE.SQAURE_PAD,\n",
    "        ),\n",
    "        proposal_generator=RPN(\n",
    "            in_features=cfg.MODEL.PROPOSAL.IN_FEATURES,\n",
    "            head=StandardRPNHead(in_channels=cfg.MODEL.PROPOSAL.HEAD.IN_CHANS, num_anchors=cfg.MODEL.PROPOSAL.HEAD.NUM_ANCHORS, conv_dims=cfg.MODEL.PROPOSAL.HEAD.CONV_DIMS),\n",
    "            anchor_generator=DefaultAnchorGenerator(\n",
    "                sizes=cfg.MODEL.PROPOSAL.ANCHOR_GENERATOR.SIZES,\n",
    "                aspect_ratios=cfg.MODEL.PROPOSAL.ANCHOR_GENERATOR.ASPECT,\n",
    "                strides=cfg.MODEL.PROPOSAL.ANCHOR_GENERATOR.STRIDES,\n",
    "                offset=0.0,\n",
    "            ),\n",
    "            anchor_matcher=Matcher(\n",
    "                thresholds=cfg.MODEL.PROPOSAL.MATCHER.THRESHOLDS, labels=cfg.MODEL.PROPOSAL.MATCHER.LABELS, allow_low_quality_matches=cfg.MODEL.PROPOSAL.MATCHER.ALLOW_LOW\n",
    "            ),\n",
    "            box2box_transform=Box2BoxTransform(weights=cfg.MODEL.PROPOSAL.BOX2BOX.WEIGHT),\n",
    "            batch_size_per_image=cfg.MODEL.PROPOSAL.BATCHSIZE,\n",
    "            positive_fraction=cfg.MODEL.PROPOSAL.POS_FRACTION,\n",
    "            pre_nms_topk=cfg.MODEL.PROPOSAL.PRE_NMS,\n",
    "            post_nms_topk=cfg.MODEL.PROPOSAL.POST_NMS,\n",
    "            nms_thresh=cfg.MODEL.PROPOSAL.NMS_THRESH,\n",
    "        ),\n",
    "        roi_heads=StandardROIHeads(\n",
    "            num_classes=cfg.MODEL.ROI_HEADS.NUM_CLASSES,\n",
    "            batch_size_per_image=cfg.MODEL.ROI_HEADS.BATCHSIZE,\n",
    "            positive_fraction=cfg.MODEL.ROI_HEADS.POS_FRACTION,\n",
    "            proposal_matcher=Matcher(\n",
    "                thresholds=cfg.MODEL.ROI_HEADS.MATCHER.THRESHOLD, labels=cfg.MODEL.ROI_HEADS.MATCHER.LABELS, allow_low_quality_matches=cfg.MODEL.ROI_HEADS.MATCHER.ALLOW_LOW\n",
    "            ),\n",
    "            box_in_features=cfg.MODEL.ROI_HEADS.IN_FEATURES,\n",
    "            box_pooler=ROIPooler(\n",
    "                output_size=cfg.MODEL.ROI_HEADS.BOX_POOLER.OUT_SIZE,\n",
    "                scales=cfg.MODEL.ROI_HEADS.BOX_POOLER.SCALES,\n",
    "                sampling_ratio=0,\n",
    "                pooler_type=\"ROIAlignV2\",\n",
    "            ),\n",
    "            box_head=FastRCNNConvFCHead(\n",
    "                input_shape=ShapeSpec(channels=cfg.MODEL.ROI_HEADS.BOX_HEAD.IN_CHANS, height=cfg.MODEL.ROI_HEADS.BOX_HEAD.HEIGHT, width=cfg.MODEL.ROI_HEADS.BOX_HEAD.WIDTH),\n",
    "                conv_dims=cfg.MODEL.ROI_HEADS.BOX_HEAD.CONV_DIMS,\n",
    "                fc_dims=cfg.MODEL.ROI_HEADS.BOX_HEAD.FC_DIMS,\n",
    "                conv_norm = cfg.MODEL.ROI_HEADS.BOX_HEAD.CONV_NORM,\n",
    "            ),\n",
    "            box_predictor=FastRCNNOutputLayers(\n",
    "                input_shape=ShapeSpec(channels=cfg.MODEL.ROI_HEADS.BOX_PREDICTOR.IN_CHANS),\n",
    "                test_score_thresh=cfg.MODEL.ROI_HEADS.BOX_PREDICTOR.TEST_SCORE,\n",
    "                test_nms_thresh=cfg.MODEL.ROI_HEADS.BOX_PREDICTOR.TEST_NMS,\n",
    "                box2box_transform=Box2BoxTransform(weights=cfg.MODEL.ROI_HEADS.BOX_PREDICTOR.BOX2BOX.WEIGHT),\n",
    "                num_classes=cfg.MODEL.ROI_HEADS.BOX_PREDICTOR.NUM_CLASSES,\n",
    "                test_topk_per_image = cfg.MODEL.ROI_HEADS.BOX_PREDICTOR.TEST_TOPK,\n",
    "                use_ncc_scores = cfg.MODEL.ROI_HEADS.BOX_PREDICTOR.NCC,\n",
    "                contrastive_train = cfg.MODEL.ROI_HEADS.BOX_PREDICTOR.CL,\n",
    "            ),\n",
    "            mask_in_features=cfg.MODEL.ROI_HEADS.MASK_FEATURES,\n",
    "        ),\n",
    "        pixel_mean=cfg.MODEL.PIXEL_MEAN,\n",
    "        pixel_std=cfg.MODEL.PIXEL_STD,\n",
    "        input_format=cfg.MODEL.INPUT_FORMAT,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(image,result,threshold=0.05,BIN_FACTOR=4):\n",
    "    image = Image.fromarray(image.astype(np.float32))\n",
    "    image = image.resize((image.width // BIN_FACTOR, image.height // BIN_FACTOR), Image.LANCZOS)\n",
    "    image = np.array(image)\n",
    "    mic = (image - image.min()) / (image.max() - image.min() + 1e-8) * 255\n",
    "    mic = mic.astype(np.uint8)\n",
    "    mic = Image.fromarray(mic)\n",
    "    mic = mic.convert('RGB')\n",
    "\n",
    "    draw = ImageDraw.Draw(mic)\n",
    "    predicted_boxes = result[0][\"instances\"].pred_boxes.tensor.cpu().numpy()\n",
    "    scores = result[0][\"instances\"].scores.cpu().numpy()\n",
    "    for i in range(len(scores)):\n",
    "        xmin, ymin, xmax, ymax = predicted_boxes[i]\n",
    "        score = scores[i]\n",
    "        if score >= threshold:\n",
    "            draw.ellipse((int(xmin) // 4, int(ymin) // 4, int(xmax) // 4, int(ymax) // 4), fill=None, outline='red', width=2)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(np.array(mic))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencer\n",
    "`DetectronInferencer` performs particle picking on one micrograph at a time and output the results. The input image will be normalized and then be resized to 1024px resolution. The output will contains the picking prediction with bounding boxes and corresponding confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectronInferencer(object):\n",
    "    def __init__(self,\n",
    "        cfg: DictConfig,\n",
    "        ckpt_path: Path,\n",
    "    ) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model = build_model(cfg).to(self.device).eval()\n",
    "        checkpoint = torch.load(ckpt_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model'])\n",
    "        self.aug = AugmentationList([T.ResizeShortestEdge(short_edge_length=1024,max_size=1024)])\n",
    "\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def inference(self, image, H, W, mean, std) -> None:\n",
    "        if image.dtype == np.float16:\n",
    "            image = image.astype(np.float32)\n",
    "        aug_input = AugInput(image, sem_seg=None)\n",
    "        transforms = self.aug(aug_input)\n",
    "        image = aug_input.image\n",
    "        image = (image - mean) / std\n",
    "        input = {}\n",
    "        input['image'] = torch.as_tensor(image).unsqueeze(0)\n",
    "        input['height'] = H\n",
    "        input['width'] = W\n",
    "        input[\"mean\"] = mean\n",
    "        input[\"std\"] = std\n",
    "        input['transforms'] = transforms.inverse()\n",
    "        results = self.model([input])\n",
    "        return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build inferencer\n",
    "To build the inferencer, provide the model parameter `.yaml` file and the corresponding checkpoint. By default, the model parameter used is `detectron_base.yaml`, which is finetuned using `DRACO-base` model. To switch to `DRACO-large`, you can change the parameter file to `detectron_large.yaml`. Note that `large` model could require a graphic card with more than 16GB display memories when inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = Path(\"detectron_base.yaml\")\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "ckpt_path = Path(\"CHECKPOINT_PATH\")\n",
    "inferencer = DetectronInferencer(cfg,ckpt_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "The network input should be normalized micrographs. By default, our data is in `.h5` format. In our customized `.h5 `data format, the mean and standard deviation of the micrograph are pre-calculated and stored in the header, allowing direct normalization of the data. For raw `.mrc` files, we have also implemented an input processing function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5 file\n",
    "img_path = \"H5_FILE_PATH\"\n",
    "with h5py.File(img_path, 'r') as hdf5_file:\n",
    "    img = hdf5_file[\"micrograph\"]\n",
    "    H,W = img.shape\n",
    "    mean = img.attrs[\"mean\"] if \"mean\" in img.attrs else img[:].astype(np.float32).mean()\n",
    "    std = img.attrs[\"std\"] if \"std\" in img.attrs else img[:].astype(np.float32).std()\n",
    "    img = img[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mrc file\n",
    "import mrcfile as mrc\n",
    "img_path = \"YOUR_MRC_FILE_PATH\"\n",
    "with mrc.open(img_path, permissive=True) as m:\n",
    "    img = m.data.copy().astype(np.float32)\n",
    "    img, mean, std = preprocess(img)\n",
    "    H,W = img.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Unlike denoising, detectron can handle the issue that input dimensions are not multiples of patch size, which is 16 in our models. After inference, we will output picking results on bin 4 image to have higher contrast. And you can adjust the confidence score threshold to achieve a reasonable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = inferencer.inference(img, H, W, mean, std)\n",
    "visualize_result(img,result,threshold=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "draco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
