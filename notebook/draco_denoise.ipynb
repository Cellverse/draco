{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising-recontruction Autoencoder (DRACO) Denoising Visualizer\n",
    "In this demo, we will show how to denoise using pretrained DRACO model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "from process import preprocess\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from draco.configuration import CfgNode\n",
    "from draco.model import (\n",
    "    build_model,\n",
    "    load_pretrained\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageType = np.ndarray | Image.Image\n",
    "\n",
    "def set_one_image(image: ImageType, title: str = \"\") -> None:\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "    image = (image-image.min()) / (image.max()-image.min())\n",
    "\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def show_one_image(image: ImageType, title: str = \"\") -> None:\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    set_one_image(image, title)\n",
    "    plt.show()\n",
    "\n",
    "def show_denoising(origin: ImageType, denoised: ImageType) -> None:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    set_one_image(origin, \"Origin\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    set_one_image(denoised, \"Denoised\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    difference = np.asarray(origin) - np.asarray(denoised)[:,:,0]\n",
    "    set_one_image(difference, \"Difference\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizer\n",
    "`DRACODenoiser` performs denoising on one micrograph at a time and then plots the result. To run inference on batched inputs or save the denoised micrograph, you can modify the `inference` function within `DRACODenoiser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRACODenoiser(object):\n",
    "    def __init__(self,\n",
    "        cfg: DictConfig,\n",
    "        ckpt_path: Path,\n",
    "    ) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.transform = self.build_transform()\n",
    "        self.model = build_model(cfg).to(self.device).eval()\n",
    "        self.model = load_pretrained(self.model, ckpt_path, self.device)\n",
    "        self.patch_size = cfg.MODEL.PATCH_SIZE\n",
    "\n",
    "    def patchify(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = image.shape\n",
    "        P = self.patch_size\n",
    "        if H % P != 0 or W % P != 0:\n",
    "            image = torch.nn.functional.pad(image, (0, (P - W % P) % P, 0, (P - H % P) % P), mode='constant', value=0)\n",
    "\n",
    "        patches = image.unfold(2, P, P).unfold(3, P, P)\n",
    "        patches = patches.permute(0, 2, 3, 4, 5, 1)\n",
    "        patches = patches.reshape(B, -1, P * P * C)\n",
    "        return patches\n",
    "\n",
    "    def unpatchify(self, patches: torch.Tensor, H: int, W: int) -> torch.Tensor:\n",
    "        B = patches.shape[0]\n",
    "        P = self.patch_size\n",
    "\n",
    "        images = patches.reshape(B, (H + P - 1) // P, (W + P - 1) // P, P, P, -1)\n",
    "        images = images.permute(0, 5, 1, 3, 2, 4)\n",
    "        images = images.reshape(B, -1, (H + P - 1) // P * P, (W + P - 1) // P * P)\n",
    "        images = images[..., :H, :W]\n",
    "        return images\n",
    "\n",
    "    @classmethod\n",
    "    def build_transform(cls) -> v2.Compose:\n",
    "        return v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True)\n",
    "        ])\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def inference(self, image: Image.Image) -> None:\n",
    "        W, H = image.size\n",
    "\n",
    "        x = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        y = self.model(x)\n",
    "\n",
    "        x = self.patchify(x).detach().cpu().numpy()\n",
    "        denoised = self.unpatchify(y, H, W).squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n",
    "\n",
    "        show_denoising(image, denoised)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Visualizer\n",
    "To build the visualizer, provide the model parameter `.yaml` file and the corresponding checkpoint. By default, the model used is `DRACO-base`. To switch to `DRACO-large`, simply change the `VIT_SCALE` parameter in `denoise.yaml` to `large`. Note that `large` model could require a graphic card with more than 16GB display memories when inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CfgNode.load_yaml_with_base(Path(\"denoise.yaml\"))\n",
    "CfgNode.merge_with_dotlist(cfg, [])\n",
    "ckpt_path = Path(\"CHECKPOINT_PATH\")\n",
    "visualizer = DRACODenoiser(cfg, ckpt_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "The network input should be normalized micrographs. By default, our data is in `.h5` format. In our customized `.h5 `data format, the mean and standard deviation of the micrograph are pre-calculated and stored in the header, allowing direct normalization of the data. For raw `.mrc` files, we have also implemented an input processing function for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5 file \n",
    "import h5py\n",
    "img_path = \"H5_FILE_PATH\"\n",
    "with h5py.File(img_path, 'r') as hdf5_file:\n",
    "    full_micrograph = hdf5_file[\"micrograph\"]\n",
    "    mean = full_micrograph.attrs[\"mean\"] if \"mean\" in full_micrograph.attrs else full_micrograph[:].astype(np.float32).mean()\n",
    "    std = full_micrograph.attrs[\"std\"] if \"std\" in full_micrograph.attrs else full_micrograph[:].astype(np.float32).std()\n",
    "    img = (hdf5_file[\"micrograph\"][:].astype(np.float32) - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mrc file\n",
    "import mrcfile as mrc\n",
    "img_path = \"YOUR_MRC_FILE_PATH\"\n",
    "with mrc.open(img_path, permissive=True) as m:\n",
    "    img = m.data.copy().astype(np.float32)\n",
    "    img, mean, std = preprocess(img)\n",
    "    img = (img - mean) / std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Set the mask ratio to 0 during inference. Since inference on a full-resolution bin 1 micrograph can be time-consuming, a 1024×1024 crop is applied in this demo, though you may input the entire micrograph if desired.\n",
    "\n",
    "Note that the input micrograph’s height and width must be multiples of the patch size, which is 16 in our models. If the micrograph dimensions do not meet this requirement, apply padding or cropping as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "visualizer.inference(Image.fromarray(img[1024:2048,1024:2048]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
