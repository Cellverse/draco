{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising-recontruction Autoencoder (DRACO) pretrained model\n",
    "In this demo, we will simply show how to load the pretrained DRACO model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "from process import preprocess\n",
    "\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from draco.configuration import CfgNode\n",
    "from draco.model import (\n",
    "    build_model,\n",
    "    load_pretrained\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models\n",
    "To load pretrained models, provide the model parameter `.yaml` file and the corresponding checkpoint. By default, the model used is `DRACO-base`. To switch to `DRACO-large`, simply change the `VIT_SCALE` parameter in `draco.yaml` to `large`. Note that `large` model could require a graphic card with more than 16GB display memories when inferencing. If you want to finetune DRACO, it would be better if you freeze the DRACO's encoder or multi-gpu clusters with large display memories are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CfgNode.load_yaml_with_base(Path(\"draco.yaml\"))\n",
    "CfgNode.merge_with_dotlist(cfg, [])\n",
    "ckpt_path = Path(\"CHECKPOINT_PATH\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(cfg).to(device).eval()\n",
    "model = load_pretrained(model, ckpt_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "draco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
